#ìµìŠ¤í”Œë¡œì–´ì–´ì— ìˆëŠ”ê±°ë¡¤ ìš”ì•½í•´ë³´ê¸°
# -*- coding: utf-8 -*-
import streamlit as st
import requests
import json
import re
import urllib.parse
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime
import uuid

# ------------------------------
# í˜ì´ì§€ ì„¤ì •
# ------------------------------
st.set_page_config(page_title="íˆ¬ìì ì„±í–¥ ë¶„ì„ ë° ê¸ˆìœµë‰´ìŠ¤ ìš”ì•½", layout="centered")
st.title("ğŸ“Š íˆ¬ìì ì„±í–¥ ë¶„ì„ & ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½ ì¶”ì²œ")

# ------------------------------
# 1ë‹¨ê³„: íˆ¬ìì ì„±í–¥ ì„¤ë¬¸
# ------------------------------
questions = [
    ("1ï¸âƒ£ íˆ¬ì ê°€ëŠ¥ ê¸°ê°„ì€?", {"A": "1ë…„ ì´í•˜", "B": "1~3ë…„", "C": "3ë…„ ì´ìƒ"}),
    ("2ï¸âƒ£ ì›ê¸ˆ ì†ì‹¤ ê°€ëŠ¥ì„±?", {"A": "ì ˆëŒ€ ë¶ˆê°€", "B": "ê°ìˆ˜ ê°€ëŠ¥", "C": "ê³ ìˆ˜ìµì´ë©´ ê°ìˆ˜"}),
    ("3ï¸âƒ£ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ”?", {"A": "ì•ˆì •ì„±", "B": "ê· í˜•", "C": "ìˆ˜ìµì„±"}),
    ("4ï¸âƒ£ ê´€ì‹¬ í‚¤ì›Œë“œ?", {"A": "ê¸ˆë¦¬, ì±„ê¶Œ", "B": "ETF, ì§€ìˆ˜", "C": "ê¸°ìˆ ì£¼, AI"}),
    ("5ï¸âƒ£ íˆ¬ì ê²½í—˜?", {"A": "ì˜ˆì ê¸ˆ", "B": "ETFÂ·í€ë“œ", "C": "ì§ì ‘ ì£¼ì‹/ì½”ì¸"}),
    ("6ï¸âƒ£ íˆ¬ì ìì‚° ë¹„ìœ¨?", {"A": "10% ì´í•˜", "B": "10~30%", "C": "30% ì´ìƒ"}),
    ("7ï¸âƒ£ ì„ í˜¸ íˆ¬ì ë°©ì‹?", {"A": "3% í™•ì • ìˆ˜ìµ", "B": "10% ê¸°ëŒ€ ìˆ˜ìµ", "C": "20% ê¸°ëŒ€, ì†ì‹¤ ê°ìˆ˜"})
]
answers = [st.radio(q, list(opts), format_func=lambda x, opts=opts: opts[x], key=f"q{i}") for i, (q, opts) in enumerate(questions)]

def classify_investor(ans):
    count = {"A": ans.count("A"), "B": ans.count("B"), "C": ans.count("C")}
    return "ğŸŸ¦ ì•ˆì •í˜• íˆ¬ìì" if count["A"] >= 4 else ("ğŸŸ¥ ê³µê²©í˜• íˆ¬ìì" if count["C"] >= 4 else "ğŸŸ¨ ì¤‘ë¦½í˜• íˆ¬ìì")

if st.button("âœ… íˆ¬ì ì„±í–¥ ë¶„ì„í•˜ê¸°"):
    investor_type = classify_investor(answers)
    st.success(f"ë‹¹ì‹ ì€ **{investor_type}** ì…ë‹ˆë‹¤!")

# ------------------------------
# 2ë‹¨ê³„: ê´€ì‹¬ í‚¤ì›Œë“œ ì„ íƒ
# ------------------------------
st.markdown("---")
st.markdown("### ğŸ” ê´€ì‹¬ ìˆëŠ” ê²½ì œ í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”")
default_keywords = ["ê¸ˆë¦¬", "ETF", "ì±„ê¶Œ", "ì£¼ì‹", "TDF", "ë¬¼ê°€", "ê²½ê¸°ì¹¨ì²´", "í™˜ìœ¨", "ì—°ì¤€", "ì¸í”Œë ˆì´ì…˜"]
selected_keywords = st.multiselect("í‚¤ì›Œë“œ ì„ íƒ (ìµœëŒ€ 3ê°œ)", default_keywords, max_selections=3)

# ------------------------------
# API KEY ì„¤ì •
# ------------------------------
CLIENT_ID = "r9LA_NX9CVuHhnhhoVOg"
CLIENT_SECRET = "kQWtIytPZX"
CLOVA_API_KEY = "nv-5e40cb6c29fc4a809841fab999de0e3dbe4f"

# ------------------------------
# ë‰´ìŠ¤ ìˆ˜ì§‘ ë° í•„í„°ë§
# ------------------------------
def clean_text(text):
    return re.sub(r"<[^>]+>|&[^;\s]+;", "", text)

def get_news(query):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    encoded_query = urllib.parse.quote(query)
    url_with_query = f"{url}?query={encoded_query}&display=20&sort=date"
    res = requests.get(url_with_query, headers=headers)
    return res.json().get("items", []) if res.status_code == 200 else []

def filter_recent_news(items, days=2):
    today = datetime.now().date()
    threshold = today - timedelta(days=days)
    filtered = []
    for item in items:
        try:
            news_date = parsedate_to_datetime(item["pubDate"]).date()
            if threshold <= news_date <= today:
                filtered.append(item)
        except:
            continue
    return filtered

def is_relevant_news(item, keywords):
    title = item.get("title", "").lower()
    desc = item.get("description", "").lower()
    for kw in keywords:
        if kw.lower() in title or kw.lower() in desc:
            return True
    return False

def crawl_news(queries, keywords):
    results = []
    for q in queries:
        for item in filter_recent_news(get_news(q)):
            if is_relevant_news(item, keywords):
                results.append({
                    "title": clean_text(item["title"]),
                    "description": clean_text(item["description"]),
                    "pubDate": item["pubDate"],
                    "link": item["link"],
                    "query": q
                })
    return results

# ------------------------------
# ìš”ì•½ API í˜¸ì¶œ
# ------------------------------
def summarize_text(text, api_key=CLOVA_API_KEY):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
        "X-NCP-CLOVASTUDIO-REQUEST-ID": str(uuid.uuid4())
    }
    data = {
        "texts": [text],
        "autoSentenceSplitter": True,
        "segCount": -1,
        "segMaxSize": 1000,
        "segMinSize": 300,
        "includeAiFilters": False
    }
    url = "https://clovastudio.stream.ntruss.com/v1/api-tools/summarization/v2"
    res = requests.post(url, headers=headers, json=data)
    if res.status_code == 200:
        return res.json()["result"]["text"]
    else:
        return f"âŒ ìš”ì•½ ì‹¤íŒ¨: {res.text}"

# ------------------------------
# 3ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìš”ì•½ ì‹¤í–‰
# ------------------------------
if selected_keywords:
    st.markdown("### ğŸ“° ì¶”ì²œ ë‰´ìŠ¤ ëª©ë¡")
    news_list = crawl_news(selected_keywords, selected_keywords)

    if news_list:
        for i, news in enumerate(news_list):
            with st.expander(f"ğŸ” {i+1}. {news['title']}"):
                st.markdown(f"- ğŸ•’ {news['pubDate']}")
                st.markdown(f"- ğŸ“„ {news['description']}")
                st.markdown(f"[ğŸ“° ì›ë¬¸ ë§í¬]({news['link']})")

                if st.button(f"ì´ ë‰´ìŠ¤ ìš”ì•½í•˜ê¸°", key=f"sum_{i}"):
                    with st.spinner("ìš”ì•½ ì¤‘..."):
                        summary = summarize_text(news["title"] + " " + news["description"])
                        st.success("ğŸ“Œ ìš”ì•½ ê²°ê³¼")
                        st.write(summary)
    else:
        st.info("ğŸ” ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
