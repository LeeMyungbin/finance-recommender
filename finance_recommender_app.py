# -*- coding: utf-8 -*-
"""Untitled53.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bUEXc2PQeqWe8JU_9UMKJ7TJwhD7dMsr
"""

import requests
import json
import os
import hashlib
from datetime import datetime
from collections import Counter
import streamlit as st

# ✅ 네이버 뉴스 API 인증 정보
CLIENT_ID = "r139rV3OFmXfYuZj2geF"
CLIENT_SECRET = "tcwu4YHfNT"
NEWS_DB_PATH = "news_data.json"

# ✅ 오늘 날짜
def today_str():
    return datetime.now().strftime("%Y-%m-%d")

# ✅ 네이버 뉴스 API 호출
def get_news(query, display=20, start=1, sort="date"):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": CLIENT_ID,
        "X-Naver-Client-Secret": CLIENT_SECRET
    }
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": sort
    }
    response = requests.get(url, headers=headers, params=params)
    if response.status_code == 200:
        return response.json()["items"]
    else:
        print("❌ 오류:", response.status_code, response.text)
        return []

# ✅ 뉴스 저장
def make_hash(title, link):
    return hashlib.md5((title + link).encode("utf-8")).hexdigest()

def load_news_db():
    if os.path.exists(NEWS_DB_PATH):
        with open(NEWS_DB_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_news_db(news_db):
    with open(NEWS_DB_PATH, "w", encoding="utf-8") as f:
        json.dump(news_db, f, ensure_ascii=False, indent=2)

def crawl_today_news(queries):
    news_db = load_news_db()
    today = today_str()
    if today not in news_db:
        news_db[today] = []
    saved_hashes = set(make_hash(n['title'], n['link']) for n in news_db[today])
    for query in queries:
        items = get_news(query)
        for item in items:
            h = make_hash(item['title'], item['link'])
            if h not in saved_hashes:
                news_db[today].append({
                    "title": item["title"],
                    "description": item["description"],
                    "pubDate": item["pubDate"],
                    "link": item["link"],
                    "query": query
                })
                saved_hashes.add(h)
    save_news_db(news_db)
    print(f"✅ {today} 뉴스 {len(news_db[today])}개 저장 완료")

# ✅ 키워드 추출
def extract_keywords(text):
    words = [w.strip('.,()[]') for w in text.split()]
    return [w for w in words if len(w) > 1 and not w.startswith("http")]

# ✅ 사용자 성향 프로필
def generate_investor_profile(survey_responses):
    risk_map = {"매우 보수적": 0.1, "보수적": 0.3, "중립": 0.5, "공격적": 0.7, "매우 공격적": 0.9}
    horizon_map = {"단기": 1, "중기": 3, "장기": 5}
    return {
        "risk_score": risk_map[survey_responses["risk_level"]],
        "horizon_years": horizon_map[survey_responses["horizon"]],
        "interest_tags": survey_responses["interests"]
    }

# ✅ 시장 테마 추출
def extract_market_themes(news_items, top_k=5):
    all_keywords = []
    for item in news_items:
        text = item['title'] + " " + item['description']
        all_keywords.extend(extract_keywords(text))
    most_common = Counter(all_keywords).most_common(top_k)
    return [word for word, _ in most_common]

# ✅ 상품 추천
def recommend_products(user_profile, market_themes, products_db):
    results = []
    for product in products_db:
        risk_gap = abs(user_profile["risk_score"] - product["risk"])
        risk_score = 1 - risk_gap
        theme_score = len(set(user_profile["interest_tags"] + market_themes) & set(product["themes"])) / (len(product["themes"]) + 1e-5)
        total_score = 0.6 * risk_score + 0.4 * theme_score
        results.append((total_score, product))
    results.sort(key=lambda x: x[0], reverse=True)
    return [prod for _, prod in results[:3]]

# ✅ Streamlit 앱 실행
st.title("맞춤형 금융상품 추천 시스템")

# (1) 사용자 설문
st.sidebar.header("투자 성향 설문")
age = st.sidebar.slider("연령대", 20, 60, 30)
risk_level = st.sidebar.selectbox("위험 성향", ["매우 보수적", "보수적", "중립", "공격적", "매우 공격적"])
horizon = st.sidebar.selectbox("투자 기간", ["단기", "중기", "장기"])
interests = st.sidebar.multiselect("관심 분야", ["인프라", "ETF", "TDF", "EMP", "스마트베타", "부동산"])

if st.sidebar.button("분석 및 추천 시작"):
    responses = {
        "age": age,
        "risk_level": risk_level,
        "horizon": horizon,
        "interests": interests
    }
    profile = generate_investor_profile(responses)

    # (2) 뉴스 수집 및 로딩
    financial_keywords = ["금리", "ETF", "구조화상품", "EMP 펀드", "리츠", "TDF", "인프라 투자", "스마트베타"]
    crawl_today_news(financial_keywords)
    with open("news_data.json", "r", encoding="utf-8") as f:
        news_db = json.load(f)
    today = max(news_db.keys())
    today_news = news_db[today]

    # (3) 시장 테마 추출
    market_themes = extract_market_themes(today_news)

    # (4) 상품 DB 예시
    products_db = [
        {"name": "글로벌 인프라 ETF", "risk": 0.7, "themes": ["인프라", "장기", "물가"], "description": "에너지, 물류, 인프라 자산에 투자하는 ETF"},
        {"name": "EMP 분산 투자 펀드", "risk": 0.5, "themes": ["ETF", "중립", "자산배분"], "description": "여러 ETF에 분산투자하여 리스크를 낮추는 펀드"},
        {"name": "스마트베타 AI ETF", "risk": 0.6, "themes": ["스마트베타", "AI", "장기"], "description": "AI 기반 포트폴리오 최적화를 활용하는 ETF"}
    ]

    # (5) 추천 수행
    recommended = recommend_products(profile, market_themes, products_db)

    st.subheader("오늘의 추천 금융 상품")
    for p in recommended:
        st.markdown(f"**{p['name']}**")
        st.markdown(f"- 설명: {p['description']}")
        st.markdown(f"- 테마: {', '.join(p['themes'])}")
        st.markdown(f"- 위험도: {p['risk']}")